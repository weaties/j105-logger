"""Audio transcription via faster-whisper with optional speaker diarisation.

Transcription runs in a thread pool to avoid blocking the event loop.
Results (including errors) are stored in the ``transcripts`` SQLite table.

Speaker diarisation is performed via pyannote.audio when ALL of the following
are true:
  1. ``HF_TOKEN`` is set in the environment.
  2. The ``pyannote.audio`` package is importable (it is NOT a hard dependency
     because it requires PyTorch which has no ARM Linux / aarch64 wheels).

Without diarisation the plain-text faster-whisper path is used.
"""

from __future__ import annotations

import asyncio
import json
import os
from typing import TYPE_CHECKING

from loguru import logger

if TYPE_CHECKING:
    from logger.storage import Storage


async def transcribe_session(
    storage: Storage,
    audio_session_id: int,
    transcript_id: int,
    model_size: str = "base",
    diarize: bool = True,
) -> None:
    """Run transcription (and optionally diarisation) and update the transcript row.

    This coroutine is intended to be launched as a background task.
    *transcript_id* must be an existing row (status='pending') created by
    ``storage.create_transcript_job()``.

    Diarisation is attempted only when *diarize* is True **and** ``HF_TOKEN``
    is present in the environment. Otherwise the plain faster-whisper path runs.
    """
    row = await storage.get_audio_session_row(audio_session_id)
    if row is None:
        await storage.update_transcript(
            transcript_id,
            status="error",
            error_msg=f"Audio session {audio_session_id} not found",
        )
        return

    await storage.update_transcript(transcript_id, status="running")
    file_path: str = row["file_path"]
    use_diarize = diarize and bool(os.environ.get("HF_TOKEN")) and _pyannote_available()

    try:
        if use_diarize:
            text, segments_json_str = await asyncio.to_thread(
                _run_with_diarization, file_path=file_path, model_size=model_size
            )
            await storage.update_transcript(
                transcript_id, status="done", text=text, segments_json=segments_json_str
            )
            logger.info(
                "Transcription+diarisation done: audio_session_id={} chars={}",
                audio_session_id,
                len(text),
            )
        else:
            text = await asyncio.to_thread(_run_whisper, file_path=file_path, model_size=model_size)
            await storage.update_transcript(transcript_id, status="done", text=text)
            logger.info(
                "Transcription done: audio_session_id={} chars={}",
                audio_session_id,
                len(text),
            )
    except Exception as exc:  # noqa: BLE001
        logger.warning("Transcription failed: audio_session_id={} err={}", audio_session_id, exc)
        await storage.update_transcript(transcript_id, status="error", error_msg=str(exc))


# ---------------------------------------------------------------------------
# Availability check
# ---------------------------------------------------------------------------


def _pyannote_available() -> bool:
    """Return True only if pyannote.audio (and torch) can be imported."""
    try:
        import pyannote.audio  # noqa: F401  # type: ignore[import-untyped]
        import torch  # noqa: F401  # type: ignore[import-untyped]

        return True
    except ImportError:
        return False


# ---------------------------------------------------------------------------
# Sync workers (run via asyncio.to_thread)
# ---------------------------------------------------------------------------


def _run_whisper(*, file_path: str, model_size: str) -> str:
    """Run faster-whisper synchronously (called from asyncio.to_thread)."""
    from faster_whisper import WhisperModel  # type: ignore[import-untyped]

    logger.debug("Loading faster-whisper model={} for {}", model_size, file_path)
    model = WhisperModel(model_size, device="cpu", compute_type="int8")
    segments, _info = model.transcribe(file_path, beam_size=5)
    parts = [seg.text for seg in segments]
    return " ".join(parts).strip()


def _run_whisper_segments(*, file_path: str, model_size: str) -> list[tuple[float, float, str]]:
    """Return [(start, end, text)] per whisper segment."""
    from faster_whisper import WhisperModel

    logger.debug("Loading faster-whisper model={} for {}", model_size, file_path)
    model = WhisperModel(model_size, device="cpu", compute_type="int8")
    segments, _info = model.transcribe(file_path, beam_size=5)
    return [(s.start, s.end, s.text) for s in segments]


def _run_diarizer(file_path: str) -> list[tuple[float, float, str]]:
    """Return [(start, end, speaker_label)] from pyannote diarisation."""
    import torch
    from pyannote.audio import Pipeline

    token = os.environ.get("HF_TOKEN") or None
    logger.debug("Loading pyannote diarization pipeline for {}", file_path)
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization-3.1", token=token)
    if pipeline is None:
        raise RuntimeError("pyannote Pipeline.from_pretrained returned None — check HF_TOKEN")
    pipeline.to(torch.device("cpu"))
    diarization = pipeline(file_path)
    return [
        (turn.start, turn.end, spk) for turn, _, spk in diarization.itertracks(yield_label=True)
    ]


def _merge(
    whisper_segs: list[tuple[float, float, str]],
    diar_segs: list[tuple[float, float, str]],
) -> list[dict[str, object]]:
    """Assign speaker to each whisper segment by midpoint lookup.

    Raw pyannote speaker labels (e.g. "SPEAKER_00", "A") are normalised to
    SPEAKER_00, SPEAKER_01, … in order of first appearance.
    """
    unique: list[str] = []
    for _, _, label in diar_segs:
        if label not in unique:
            unique.append(label)
    label_map = {lbl: f"SPEAKER_{i:02d}" for i, lbl in enumerate(unique)}

    def speaker_at(mid: float) -> str:
        for s, e, lbl in diar_segs:
            if s <= mid <= e:
                return label_map[lbl]
        # fallback: nearest interval
        if not diar_segs:
            return "SPEAKER_00"
        nearest = min(diar_segs, key=lambda x: min(abs(x[0] - mid), abs(x[1] - mid)))
        return label_map[nearest[2]]

    return [
        {"start": s, "end": e, "speaker": speaker_at((s + e) / 2), "text": t}
        for s, e, t in whisper_segs
    ]


def _run_with_diarization(*, file_path: str, model_size: str) -> tuple[str, str]:
    """Run whisper + diarisation and return (plain_text, segments_json)."""
    whisper_segs = _run_whisper_segments(file_path=file_path, model_size=model_size)
    diar_segs = _run_diarizer(file_path)
    segments = _merge(whisper_segs, diar_segs)
    plain = "\n".join(f"{seg['speaker']}: {str(seg['text']).strip()}" for seg in segments)
    return plain, json.dumps(segments)
